https://karuppiah7890.github.io/blog/posts/kubernetes-jobs-and-cronjobs-do-you-need-them-why/

A job retries pods until they complete, so that you can tolerate errors that cause pods to be deleted.

If you want to run a job repeatedly and periodically, you can use CronJob alpha or cronetes.

Some Helm Charts use Jobs to run install, setup, or test commands on clusters, as part of installing services. (Example).

If you save the YAML for the job then you can re-run it by deleting the old job an creating it again, or 
by editing the YAML to change the name (or use e.g. sed in a script). 

One of the use case can be to take a backup of a DB. But as already mentioned that are some overheads to run a job e.g.
When a Job completes the Pods are not deleted . so you need to manually delete the job(which will also delete the pods created by job). 
so recommended option will be to use Cron instead of Jobs 

===================
As the Kubernetes documentation explains, a Kubernetes Job creates one or more pods and ensures that a specified number of the pods terminates
when the task (Job) completes.

Just like in a typical operating system, the ability to perform automated,
scheduled jobs without user interaction is important in the Kubernetes world. 
But Kubernetes Jobs do more than just run automated jobs, and there are multiple ways to utilize them through:

Each of these features has its own purpose, with the common function to ensure that pods run continuously. 
In failure scenarios, these controllers either restart or reschedule pods to ensure the services in the pods continue running.

Jobs play an important role in Kubernetes, especially for running batch processes
Jobs differ from other Kubernetes controllers in that they run tasks until completion, 
rather than managing the desired state such as in Deployments, ReplicaSets, and StatefulSets.

A CronJob the object allows you to schedule Job execution rather than starting them manually.

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *"   -o yaml --dry-run=client -- /bin/sh -c 'date; echo Hello from the Kubernetes cluster'

kubectl create job1 rav --image=busybox -o yaml  --dry-run=client   -- /bin/sh -c 'date; echo sleeping....; sleep 90s; echo exiting...'

Mutlicontainer
=============

The 1st container runs nginx server and has the shared volume mounted to the directory /usr/share/nginx/html.
The 2nd container uses the Debian image and has the shared volume mounted to the directory /html.
Every second, the 2nd container adds the current date and time into the index.html file, which is located in the shared volume.
When the user makes an HTTP request to the Pod, the Nginx server reads this file and transfers it back to the user in response to the request.



kubectl exec -it mc1 -c web -- /bin/sh
 kubectl exec -it mc1 -c web -- /bin/sh





statefulset:
===========

https://www.magalix.com/blog/kubernetes-statefulsets-101-state-of-the-pods
https://github.com/collabnix/kubelabs/blob/master/StatefulSets101/README.md#what-is-statefulset-and-how-is-it-different-from-deployment

https://medium.com/stakater/k8s-deployments-vs-statefulsets-vs-daemonsets-60582f0c62d4
https://medium.com/avmconsulting-blog/deploying-statefulsets-in-kubernetes-k8s-5924e701d327

statefulset:
==============

zookeeper:
master and follower method
i have 3 zookeeper(pods)
1 master 2 followers
first it need to pass infromation or metadata will go master and then it should go to followers
zk0,zk1,zk2 deploy
1)when create deploy all pods get deployed all at once not a statefulset(kafa)

in mysql first master node deploy master info slave1 -> slave2 -> slave3
in statefulset it deploy one by one 

in deployment when pod create podname(hostname of pod) and ip is dynamic
using stateful set we can create static hostname and ip

master1 write to data - > mysq10 and msql0 to replicate  to msql1 and msql2. 
when create service it writes to all at once
so we need headless service
it will point to dns server msq0 we dont give any ip address
so it will write to msql to msql2 msql2 because it point to head but msqls read to from master



https://katharharshal1.medium.com/kubernetes-job-cronjob-c1ebfc46273d

https://katharharshal1.medium.com/kubernetes-job-cronjob-c1ebfc46273d

Jobs:

Now we see different types of job parameters.

    Completions.
    parallelism.
    backofflimit.
    activeDeadlineSecond
    
Completions

    completions: Basically the repetition of a process, default it’s 1.

e.x: if we set completion as value 2, the jobs runes 2times.

2. In this pod runes one after another, when 1st pod completed then 2nd pod is started. it’s also called sequential processing.

========

parallelism

    In completions, we see jobs runners sequential one after another.
    Using the parallelism we can set the limit that how many jobs can run in parallel

e.x: if u set completion: 2, and parallelism:2 both the pods are running at the same time

=====


backoffLimit

    If the container is failing to load the image or completing the job, then still it creates more pods one after another until it succeeds.
    In this situation, backoffLimit is used.
    We set the backoffLimit for the job , that not create more pods once it’s failed.

E.X: backoffLimit:2, once pods failed 2 times it won’t create more pods.

==========

activeDeadlineSecond

    In we can set a threshold to say, if I know job taking 10 sec to come up, so I set an activeDeadlineSecond as a 10second.
    If I set activeDeadlineSecond: 10s, then if pods taking more time then the 10second then pods are automatically terminated.
    
    



Readiness & Liveness in PODS:
==============================

This is used for testing. Before the POD availabe readinees probe will check wheather your defined service is ready or not . if running  Pod gets available 

Liveneess is like after pod gets started and check the service defined is running or not. like it continous probe the service

To perform a probe, the kubelet executes the command cat /tmp/healthy in the target container

The initialDelaySeconds field tells the kubelet that it should wait 5 seconds before performing the first probe

The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds.

If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. 
If the command returns a non-zero value, the kubelet kills the container and restarts it.




apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    readinessProbe:
     exec:
      command:
         - cat
         - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
  restartPolicy: Always
  
  
  
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    readinessProbe:
     exec:
      command:
         - touch
         - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
    livenessProbe:
     exec:
        command:
        - cat
        - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
  restartPolicy: Always

  
  
  configMap
  ==========
  
  
  There are 2 phases involved in configuring ConfigMaps.

    First, create the configMaps
    Second, Inject then into the pod.

There are 2 ways of creating a configmap.

    The Imperative way
    
 $kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
$ kubectl create configmap app-config --from-file=app_config.properties (Another way)


  
  kubectl create cm morevar --from-literal=mysql=passwodofme --from-literal=var3=vzg --dry-run=client -o yaml

  kubectl create cm mycm --from-file=variables --dry-run=client -o yaml > a.yml
  
    containers:
  - image: nginx
    name: pod1
    envFrom:
      - configMapRef:
          name: mycm

  ================================================================================================
  
 kubectl create secret generic mysecret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-literal=passphrase=password
 
 kubectl create secret generic mysecret --from-literal=username=ravikiran --from-literal=passphrase=password --dry-run=client -o yaml > sec.yaml   

  echo  'cmF2aQ==' | base64 -d
  echo -n 'ravi' | base64
  
  
   - image: nginx
    name: pod2
    envFrom:
      - secretRef:
          name: mysecret
   
   =============================
  envFrom:
      - secretRef:
          name: mysecret
  
  envFrom:
      - configMapRef:
          name: filename
          
 
 
 kubectl create secret generic mysecret --from-literal=username=ravikiran --from-literal=passphrase=something

apiVersion: v1
kind: Pod
metadata:
  name: secret-test-pod
spec:
  containers:
    - name: test-container
      image: nginx
      envFrom:
      - secretRef:
          name: mysecret
  restartPolicy: Never
  
  
  apiVersion: v1
kind: Pod
metadata:
  name: sec1
spec:
  containers:
  - image: nginx
    name: nginx1
    volumeMounts:
    - mountPath: "/etc/foo"
      name: foo
  volumes:
  - name: foo
    secret:
      secretName: mysecret
  restartPolicy: Always

==========================================================================
apiVersion: v1
kind: Pod
metadata:
  name: sec
spec:
  containers:
  - image: nginx
    name: sec
    env:
      - name: MYSQL_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysql
            key: password
==================================================================================

targetport: the port the container is listening on

port number : port number exposed internally inside a cluster

A Service enables network access to a set of Pods in Kubernetes.


kubectl create  deploy web  --image=nginx --dry-run=client -o yaml > web.yaml

kubectl expose deploy/web --type=ClusterIP --port=80 --dry-run=client -o yaml > clusterip.yaml

kubectl expose deployment/web  --type=LoadBalancer --port=80 --dry-run=client -o yaml > loadbalancer.yaml

kubectl expose deployment/web  --type=NodePort --port=80 --dry-run=client -o yaml > nodeport.yaml

. ClusterIP is the default and most common service type.
 Kubernetes will assign a cluster-internal IP address to ClusterIP service.
  This  makes the  service only reachable within the cluster.
 You cannot make requests to service (pods) from outside the cluster. 
 Inter service communication within the cluster


This service visible outside the Kubernetes cluster by the node’s IP address and the port number declared . 
The service also has  to be of type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically). 
Node port must be in the range of 30000–32767. Manually allocating a port to the service is optional



 Every time you want to expose a service to the outside world, you have  to create a new        LoadBalancer  and get an IP address.
 It exposes the service externally using a cloud providers load Balancer
 Each cloud provider (AWS, Azure, GCP,) has its own native load balancer implementation. The cloud provider will create a load balancer, which then automatically routes requests to your Kubernetes Service.






kubectl create  deploy web  --image=nginx --dry-run=client -o yaml > web.yaml

kubectl expose deploy/web --type=ClusterIP --port=80 --dry-run=client -o yaml > clusterip.yaml

kubectl expose deployment/web  --type=LoadBalancer --port=80 --dry-run=client -o yaml > loadbalancer.yaml

kubectl expose deployment/web  --type=NodePort --port=80 --dry-run=client -o yaml > nodeport.yaml


https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#populate-a-volume-with-data-stored-in-a-configmap

apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: nginx
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        # Provide the name of the ConfigMap containing the files you want
        # to add to the container
        name: mycm
  
  
 why u need volumes in kubernetes
 u have database application what need to store data.
 has u know about deployments and replicaset and  it will create new pods when pods deletes or restarts.so the data is lost.
 so in order to store this data you need volumes.
 
 In this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster.
 A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.
 
 In a production cluster, you would not use hostPath. 
 Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, 
 or an Amazon Elastic Block Store volume.
 Cluster administrators can also use StorageClasses to set up dynamic provisioning.
 
 
requirments for storage:
1)storage should be independent pods lifecycle.
2)storage should be accessable to all nodes
3)storage should survive cluster crashes



persistance voulme is just like other resource like memory,ram and cpu 

all the you define on your volumes differs from cloud to cloud

it is recommned to have remote storge than local stoarge becuase it will available to all nodes with in cluster.
administrator will create pv
devlopers who uses pv will create pvc
u just claiming what ever persistant volumes u have.

The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims to request physical storage.
In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at least one Node.

Here is the configuration file for the PersistentVolumeClaim:


storage class is a way to provision persistent volumes dynamically
in yaml you define in remote persistant volume and claim it.

sudo mkdir /mnt/data

ReadWriteOnce, which means the volume can be mounted as read-write by a single Node.

It defines the StorageClass name manual for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.

When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim.
Currently, volumes can either be Retained, Recycled, or Deleted.

Reclaim Policy

Current reclaim policies are:

    Retain -- manual reclamation
    Recycle -- basic scrub (rm -rf /thevolume/*)
    Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.


https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

sudo sh -c "echo 'Hello from Kubernetes storage' > /mnt/data/index.html"

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi

=====================================================================================


  
  kubectl explain pod.spec.volumes
  
  An emptyDir volume is first created when a Pod is assigned to a Node and exists as long  as that Pod is running on that node.
. As the name says, it is initially empty. All Containers in the same Pod can read and write in the same emptyDir volume.
. When a Pod is restarted or removed, the data in the emptyDir is lost forever.

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: ngnix1
  name: ngnix1
spec:
  containers:
  - image: nginx
    name: ngnix
    volumeMounts:
      - mountPath: /test-pd
        name: test-volume
  volumes:
  - name: test-volume
    hostPath:
    path: /data
    
    
  sudo apt-get update
sudo apt-get install procps
ps aux



. A hostPath volume mounts a file or directory from the node's filesystem into the Pod.
You can specify whether the file/directory must already exist on the node or should be created on pod startup.
You can do it using a type attribute in the config file:


apiVersion: v1
kind: Pod
metadata:
  labels:
    run: ngnix
  name: ngnix
spec:
  containers:
  - image: nginx
    name: ngnix
    volumeMounts:
      - mountPath: /test
        name: test
  volumes:
      - name: test
        emptyDir: {}
        
       




 Persistent Volume (PV) is an abstraction for the physical storage device that you have attached to the cluster. Pods can use this storage space using Persistent Volume Claims (PVC).
 PV is an abstraction for the physical storage device that attached to the cluster. PV is used to manage durable storage which needs actual physical storage.
 PV is independent of the lifecycle of the Pods. It means that data represented by a PV continue to exist as the cluster changes and as Pods are deleted and recreated.
 PV is not Namespaced, it is available to whole cluster. i.e PV is accessible to all namespaces.

 Kubernetes looks for a PV that meets the criteria defined in the PVC, and if there is one, it matches claim to PV.
 PVC must be in same namespace as the Pod. For each Pod, a PVC makes a storage consumption request within a namespace.
 
 apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data
    
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  



  
  
  
  configMap
  ==========
  
  
  There are 2 phases involved in configuring ConfigMaps.

    First, create the configMaps
    Second, Inject then into the pod.

There are 2 ways of creating a configmap.

    The Imperative way
    
 $kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
$ kubectl create configmap app-config --from-file=app_config.properties (Another way)


  
  kubectl create cm morevar --from-literal=mysql=passwodofme --from-literal=var3=vzg --dry-run=client -o yaml

  kubectl create cm mycm --from-file=variables --dry-run=client -o yaml > a.yml
  
    containers:
  - image: nginx
    name: pod1
    envFrom:
      - configMapRef:
          name: mycm

  ================================================================================================
  
 kubectl create secret generic mysecret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-literal=passphrase=password
 
 kubectl create secret generic mysecret --from-literal=username=ravikiran --from-literal=passphrase=password --dry-run=client -o yaml > sec.yaml   

  echo  'cmF2aQ==' | base64 -d
  echo -n 'ravi' | base64
  
  
   - image: nginx
    name: pod2
    envFrom:
      - secretRef:
          name: mysecret
   
   =============================
  envFrom:
      - secretRef:
          name: mysecret
  
  envFrom:
      - configMapRef:
          name: filename
          
      ============================
          
       
   
  =================================================================================================   
          

Nodeselectors:

Generally, such constraints are not required, as our scheduler is intelligent enough to automatically do a reasonable placement to avoid placing the pod
on a node with insufficient free resources, but there are some instances where you may want more control on a node where a pod lands, for example

The same applies to our Kubernetes worker nodes. If you will try to schedule a heavy workload to the worker node of low capacity,
there is a higher probability of this node filling fast and create inefficiency in the cluster. 
So as a Kubernetes administrator or developer what you should do?

You may want to ensure that any given pod should end up on a machine with an SSD attached to it
Or You may desire to co-locate pods from two different services that communicate a lot into the same availability zone

The scheduler uses these labels to match and identify the right node to place the pods on.

kubectl label nodes node-1 size=Large
kubectl describe nodes



apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: data-processor
   image: data-processor
 nodeSelector:
  size: Large
          
          
Node Selector - Limitations

We used a single label and selector to achieve our goal here. But what if our requirement is much more complex.

  For this we have Node Affinity and Anti Affinity

 
 
========================================================================================================================

Taints and Tolearations:

taints and toleration are like what pods to be schudled on nodes

taints are set on nodes and tolerations are set on pods


taint: is the command to apply taints in the nodes
nodes: are set of worker nodes
nodename: is the name of the specific worker node, on which taint has to be applied, it has a key-value pair
key-value pair: it is used to specify which application type in the pod will this node may be attached
taint-effect: This is used to define how the pod will be treated if they are not tolerant of the taint.

The effects are as below;

    NoSchedule — Pods will not be schedule on the nodes
    PreferNoSchedule — The system will try to avoid placing a pod on the node, but it’s not guaranteed
    NoExecute — New pods will not be scheduled on the node and existing pods on the node if any will be evicted if they do not tolerate the taint
  












Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule


kubectl taint nodes node1 app=blue:NoSchedule















Requests define the minimum amount of resources that containers need.

If you think that your app requires at least 256MB of memory to operate, this is the request value.

The application can use more than 256MB, but Kubernetes guarantees a minimum of 256MB to the container.

On the other hand, limits define the max amount of resources that the container can consume.

Your application might require at least 256MB of memory, but you might want to be sure that it doesn't consume more than 1GB of memory.

That's your limit.

Notice how your application has 256MB of memory guaranteed, but it can grow up until 1GB of memory.

After that, it is stopped or throttled by Kubernetes


kubectl autoscale rs web --max=5

 kubectl autoscale deploy demo --cpu-percent=50  --min=1 --max=5 --dry-run=client -o yaml
 
 kubectl run v --image=nginx --requests='cpu=50m,memory=50Mi' --limits='cpu=50m,memory=50Mi'  --dry-run=client -o yaml > b.yml
 
  kubectl autoscale deploy demo --cpu-percent=50  --min=1 --max=5 --dry-run=client -o yaml
 
 stress --cpu 3
 
 apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  strategy: {}
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - image: httpd
        name: httpd
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m

cpuutilazation:20
