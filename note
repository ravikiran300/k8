  
  kubectl explain pod.spec.volumes
  
  An emptyDir volume is first created when a Pod is assigned to a Node and exists as long  as that Pod is running on that node.
. As the name says, it is initially empty. All Containers in the same Pod can read and write in the same emptyDir volume.
. When a Pod is restarted or removed, the data in the emptyDir is lost forever.

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: ngnix1
  name: ngnix1
spec:
  containers:
  - image: nginx
    name: ngnix
    volumeMounts:
      - mountPath: /test-pd
        name: test-volume
  volumes:
  - name: test-volume
    hostPath:
    path: /data



. A hostPath volume mounts a file or directory from the node's filesystem into the Pod.
You can specify whether the file/directory must already exist on the node or should be created on pod startup.
You can do it using a type attribute in the config file:


apiVersion: v1
kind: Pod
metadata:
  labels:
    run: ngnix
  name: ngnix
spec:
  containers:
  - image: nginx
    name: ngnix
    volumeMounts:
      - mountPath: /test
        name: test
  volumes:
      - name: test
        emptyDir: {}
        
       

 Persistent Volume (PV) is an abstraction for the physical storage device that you have attached to the cluster. Pods can use this storage space using Persistent Volume Claims (PVC).
 PV is an abstraction for the physical storage device that attached to the cluster. PV is used to manage durable storage which needs actual physical storage.
 PV is independent of the lifecycle of the Pods. It means that data represented by a PV continue to exist as the cluster changes and as Pods are deleted and recreated.
 PV is not Namespaced, it is available to whole cluster. i.e PV is accessible to all namespaces.

 Kubernetes looks for a PV that meets the criteria defined in the PVC, and if there is one, it matches claim to PV.
 PVC must be in same namespace as the Pod. For each Pod, a PVC makes a storage consumption request within a namespace.
 
 apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data
    
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  



  
  
  
  configMap
  ==========
  
  
  There are 2 phases involved in configuring ConfigMaps.

    First, create the configMaps
    Second, Inject then into the pod.

There are 2 ways of creating a configmap.

    The Imperative way
    
 $kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
$ kubectl create configmap app-config --from-file=app_config.properties (Another way)


  
  kubectl create cm morevar --from-literal=mysql=passwodofme --from-literal=var3=vzg --dry-run=client -o yaml

  kubectl create cm mycm --from-file=variables --dry-run=client -o yaml > a.yml
  
    containers:
  - image: nginx
    name: pod1
    envFrom:
      - configMapRef:
          name: mycm

  ================================================================================================
  
 kubectl create secret generic mysecret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-literal=passphrase=password
 
 kubectl create secret generic mysecret --from-literal=username=ravikiran --from-literal=passphrase=password --dry-run=client -o yaml > sec.yaml   

  echo  'cmF2aQ==' | base64 -d
  echo -n 'ravi' | base64
  
  
   - image: nginx
    name: pod2
    envFrom:
      - secretRef:
          name: mysecret
   
   =============================
  envFrom:
      - secretRef:
          name: mysecret
  
  envFrom:
      - configMapRef:
          name: filename
          
      ============================
          
       
   
  =================================================================================================   
          

Nodeselectors:

Generally, such constraints are not required, as our scheduler is intelligent enough to automatically do a reasonable placement to avoid placing the pod
on a node with insufficient free resources, but there are some instances where you may want more control on a node where a pod lands, for example

The same applies to our Kubernetes worker nodes. If you will try to schedule a heavy workload to the worker node of low capacity,
there is a higher probability of this node filling fast and create inefficiency in the cluster. 
So as a Kubernetes administrator or developer what you should do?

You may want to ensure that any given pod should end up on a machine with an SSD attached to it
Or You may desire to co-locate pods from two different services that communicate a lot into the same availability zone

The scheduler uses these labels to match and identify the right node to place the pods on.

kubectl label nodes node-1 size=Large
kubectl describe nodes



apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: data-processor
   image: data-processor
 nodeSelector:
  size: Large
          
          
Node Selector - Limitations

We used a single label and selector to achieve our goal here. But what if our requirement is much more complex.

  For this we have Node Affinity and Anti Affinity

 
 
========================================================================================================================

Taints and Tolearations:

taints and toleration are like what pods to be schudled on nodes

taints are set on nodes and tolerations are set on pods


taint: is the command to apply taints in the nodes
nodes: are set of worker nodes
nodename: is the name of the specific worker node, on which taint has to be applied, it has a key-value pair
key-value pair: it is used to specify which application type in the pod will this node may be attached
taint-effect: This is used to define how the pod will be treated if they are not tolerant of the taint.

The effects are as below;

    NoSchedule — Pods will not be schedule on the nodes
    PreferNoSchedule — The system will try to avoid placing a pod on the node, but it’s not guaranteed
    NoExecute — New pods will not be scheduled on the node and existing pods on the node if any will be evicted if they do not tolerate the taint
  












Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule


kubectl taint nodes node1 app=blue:NoSchedule















Requests define the minimum amount of resources that containers need.

If you think that your app requires at least 256MB of memory to operate, this is the request value.

The application can use more than 256MB, but Kubernetes guarantees a minimum of 256MB to the container.

On the other hand, limits define the max amount of resources that the container can consume.

Your application might require at least 256MB of memory, but you might want to be sure that it doesn't consume more than 1GB of memory.

That's your limit.

Notice how your application has 256MB of memory guaranteed, but it can grow up until 1GB of memory.

After that, it is stopped or throttled by Kubernetes


kubectl autoscale rs web --max=5

 kubectl autoscale deploy demo --cpu-percent=50  --min=1 --max=5 --dry-run=client -o yaml
 
 kubectl run v --image=nginx --requests='cpu=50m,memory=50Mi' --limits='cpu=50m,memory=50Mi'  --dry-run=client -o yaml > b.yml
 
  kubectl autoscale deploy demo --cpu-percent=50  --min=1 --max=5 --dry-run=client -o yaml
 
 stress --cpu 3
 
 apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  strategy: {}
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - image: httpd
        name: httpd
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m

cpuutilazation:20
