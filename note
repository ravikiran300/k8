This is used for testing. Before the POD availabe readinees probe will check wheather your defined service is ready or not . if running  Pod gets available 

Liveneess is like after pod gets started and check the service defined is running or not. like it continous probe the service


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    readinessProbe:
     exec:
      command:
         - cat
         - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
  restartPolicy: Always
  
  
  
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    readinessProbe:
     exec:
      command:
         - touch
         - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
    livenessProbe:
     exec:
        command:
        - cat
        - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5
  restartPolicy: Always




targetport: the port the container is listening on

port number : port number exposed internally inside a cluster

A Service enables network access to a set of Pods in Kubernetes.


kubectl create  deploy web  --image=nginx --dry-run=client -o yaml > web.yaml

kubectl expose deploy/web --type=ClusterIP --port=80 --dry-run=client -o yaml > clusterip.yaml

kubectl expose deployment/web  --type=LoadBalancer --port=80 --dry-run=client -o yaml > loadbalancer.yaml

kubectl expose deployment/web  --type=NodePort --port=80 --dry-run=client -o yaml > nodeport.yaml

. ClusterIP is the default and most common service type.
 Kubernetes will assign a cluster-internal IP address to ClusterIP service.
  This  makes the  service only reachable within the cluster.
 You cannot make requests to service (pods) from outside the cluster. 
 Inter service communication within the cluster


This service visible outside the Kubernetes cluster by the node’s IP address and the port number declared . 
The service also has  to be of type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically). 
Node port must be in the range of 30000–32767. Manually allocating a port to the service is optional



 Every time you want to expose a service to the outside world, you have  to create a new        LoadBalancer  and get an IP address.
 It exposes the service externally using a cloud providers load Balancer
 Each cloud provider (AWS, Azure, GCP,) has its own native load balancer implementation. The cloud provider will create a load balancer, which then automatically routes requests to your Kubernetes Service.






kubectl create  deploy web  --image=nginx --dry-run=client -o yaml > web.yaml

kubectl expose deploy/web --type=ClusterIP --port=80 --dry-run=client -o yaml > clusterip.yaml

kubectl expose deployment/web  --type=LoadBalancer --port=80 --dry-run=client -o yaml > loadbalancer.yaml

kubectl expose deployment/web  --type=NodePort --port=80 --dry-run=client -o yaml > nodeport.yaml


https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#populate-a-volume-with-data-stored-in-a-configmap

apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: nginx
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        # Provide the name of the ConfigMap containing the files you want
        # to add to the container
        name: mycm
  
  
 why u need volumes in kubernetes
 u have database application what need to store data.
 has u know about deployments and replicaset and  it will create new pods when pods deletes or restarts.so the data is lost.
 so in order to store this data you need volumes.
 
 In this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster.
 A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.
 
 In a production cluster, you would not use hostPath. 
 Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, 
 or an Amazon Elastic Block Store volume.
 Cluster administrators can also use StorageClasses to set up dynamic provisioning.
 
 
requirments for storage:
1)storage should be independent pods lifecycle.
2)storage should be accessable to all nodes
3)storage should survive cluster crashes



persistance voulme is just like other resource like memory,ram and cpu 

all the you define on your volumes differs from cloud to cloud

it is recommned to have remote storge than local stoarge becuase it will available to all nodes with in cluster.
administrator will create pv
devlopers who uses pv will create pvc
u just claiming what ever persistant volumes u have.

The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims to request physical storage.
In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at least one Node.

Here is the configuration file for the PersistentVolumeClaim:


storage class is a way to provision persistent volumes dynamically
in yaml you define in remote persistant volume and claim it.

sudo mkdir /mnt/data

ReadWriteOnce, which means the volume can be mounted as read-write by a single Node.

It defines the StorageClass name manual for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.

When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim.
Currently, volumes can either be Retained, Recycled, or Deleted.

Reclaim Policy

Current reclaim policies are:

    Retain -- manual reclamation
    Recycle -- basic scrub (rm -rf /thevolume/*)
    Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.


https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

sudo sh -c "echo 'Hello from Kubernetes storage' > /mnt/data/index.html"

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi

=====================================================================================


  
  kubectl explain pod.spec.volumes
  
  An emptyDir volume is first created when a Pod is assigned to a Node and exists as long  as that Pod is running on that node.
. As the name says, it is initially empty. All Containers in the same Pod can read and write in the same emptyDir volume.
. When a Pod is restarted or removed, the data in the emptyDir is lost forever.

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: ngnix1
  name: ngnix1
spec:
  containers:
  - image: nginx
    name: ngnix
    volumeMounts:
      - mountPath: /test-pd
        name: test-volume
  volumes:
  - name: test-volume
    hostPath:
    path: /data
    
    
  sudo apt-get update
sudo apt-get install procps
ps aux



. A hostPath volume mounts a file or directory from the node's filesystem into the Pod.
You can specify whether the file/directory must already exist on the node or should be created on pod startup.
You can do it using a type attribute in the config file:


apiVersion: v1
kind: Pod
metadata:
  labels:
    run: ngnix
  name: ngnix
spec:
  containers:
  - image: nginx
    name: ngnix
    volumeMounts:
      - mountPath: /test
        name: test
  volumes:
      - name: test
        emptyDir: {}
        
       




 Persistent Volume (PV) is an abstraction for the physical storage device that you have attached to the cluster. Pods can use this storage space using Persistent Volume Claims (PVC).
 PV is an abstraction for the physical storage device that attached to the cluster. PV is used to manage durable storage which needs actual physical storage.
 PV is independent of the lifecycle of the Pods. It means that data represented by a PV continue to exist as the cluster changes and as Pods are deleted and recreated.
 PV is not Namespaced, it is available to whole cluster. i.e PV is accessible to all namespaces.

 Kubernetes looks for a PV that meets the criteria defined in the PVC, and if there is one, it matches claim to PV.
 PVC must be in same namespace as the Pod. For each Pod, a PVC makes a storage consumption request within a namespace.
 
 apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data
    
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  



  
  
  
  configMap
  ==========
  
  
  There are 2 phases involved in configuring ConfigMaps.

    First, create the configMaps
    Second, Inject then into the pod.

There are 2 ways of creating a configmap.

    The Imperative way
    
 $kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
$ kubectl create configmap app-config --from-file=app_config.properties (Another way)


  
  kubectl create cm morevar --from-literal=mysql=passwodofme --from-literal=var3=vzg --dry-run=client -o yaml

  kubectl create cm mycm --from-file=variables --dry-run=client -o yaml > a.yml
  
    containers:
  - image: nginx
    name: pod1
    envFrom:
      - configMapRef:
          name: mycm

  ================================================================================================
  
 kubectl create secret generic mysecret --from-file=ssh-privatekey=/root/.ssh/id_rsa --from-literal=passphrase=password
 
 kubectl create secret generic mysecret --from-literal=username=ravikiran --from-literal=passphrase=password --dry-run=client -o yaml > sec.yaml   

  echo  'cmF2aQ==' | base64 -d
  echo -n 'ravi' | base64
  
  
   - image: nginx
    name: pod2
    envFrom:
      - secretRef:
          name: mysecret
   
   =============================
  envFrom:
      - secretRef:
          name: mysecret
  
  envFrom:
      - configMapRef:
          name: filename
          
      ============================
          
       
   
  =================================================================================================   
          

Nodeselectors:

Generally, such constraints are not required, as our scheduler is intelligent enough to automatically do a reasonable placement to avoid placing the pod
on a node with insufficient free resources, but there are some instances where you may want more control on a node where a pod lands, for example

The same applies to our Kubernetes worker nodes. If you will try to schedule a heavy workload to the worker node of low capacity,
there is a higher probability of this node filling fast and create inefficiency in the cluster. 
So as a Kubernetes administrator or developer what you should do?

You may want to ensure that any given pod should end up on a machine with an SSD attached to it
Or You may desire to co-locate pods from two different services that communicate a lot into the same availability zone

The scheduler uses these labels to match and identify the right node to place the pods on.

kubectl label nodes node-1 size=Large
kubectl describe nodes



apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: data-processor
   image: data-processor
 nodeSelector:
  size: Large
          
          
Node Selector - Limitations

We used a single label and selector to achieve our goal here. But what if our requirement is much more complex.

  For this we have Node Affinity and Anti Affinity

 
 
========================================================================================================================

Taints and Tolearations:

taints and toleration are like what pods to be schudled on nodes

taints are set on nodes and tolerations are set on pods


taint: is the command to apply taints in the nodes
nodes: are set of worker nodes
nodename: is the name of the specific worker node, on which taint has to be applied, it has a key-value pair
key-value pair: it is used to specify which application type in the pod will this node may be attached
taint-effect: This is used to define how the pod will be treated if they are not tolerant of the taint.

The effects are as below;

    NoSchedule — Pods will not be schedule on the nodes
    PreferNoSchedule — The system will try to avoid placing a pod on the node, but it’s not guaranteed
    NoExecute — New pods will not be scheduled on the node and existing pods on the node if any will be evicted if they do not tolerate the taint
  












Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule


kubectl taint nodes node1 app=blue:NoSchedule















Requests define the minimum amount of resources that containers need.

If you think that your app requires at least 256MB of memory to operate, this is the request value.

The application can use more than 256MB, but Kubernetes guarantees a minimum of 256MB to the container.

On the other hand, limits define the max amount of resources that the container can consume.

Your application might require at least 256MB of memory, but you might want to be sure that it doesn't consume more than 1GB of memory.

That's your limit.

Notice how your application has 256MB of memory guaranteed, but it can grow up until 1GB of memory.

After that, it is stopped or throttled by Kubernetes


kubectl autoscale rs web --max=5

 kubectl autoscale deploy demo --cpu-percent=50  --min=1 --max=5 --dry-run=client -o yaml
 
 kubectl run v --image=nginx --requests='cpu=50m,memory=50Mi' --limits='cpu=50m,memory=50Mi'  --dry-run=client -o yaml > b.yml
 
  kubectl autoscale deploy demo --cpu-percent=50  --min=1 --max=5 --dry-run=client -o yaml
 
 stress --cpu 3
 
 apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myapp
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  strategy: {}
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - image: httpd
        name: httpd
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m

cpuutilazation:20
