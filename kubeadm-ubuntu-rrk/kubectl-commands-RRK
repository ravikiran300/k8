Author : Ravi Ravada

=============================================================================================================================================

POD:

kubectl run busybox --image=busybox --restart=Never -o yaml --dry-run=client  -- /bin/sh -c   'while true;do echo amobee;sleep  5;done' > pod1.yaml

 kubectl logs  -f busybox -c busybox

kubectl exec busybox -c busybox -- ls

 kubectl run rvada --image=vadisala/rvadisala_docker_repo:nginx-custom
==================================================================
Namespaces
============

kubectl create namespace mynamespace

kubectl config view | grep namespaces

 kubectl api-resources  --namespaced=false
 
#TO SET CURRENT NAMESPACE
kubectl config set-context --current --namespace=prod

kubectl create namespace mynamespace

kubectl run nginx --image=nginx --restart=Never -n mynamespace

 kubectl run nginx2 --image=nginx --port=80  --dry-run=client -o yaml > namespae.yml

kubectl get po --all-namespaces
kubectl run nginx --image=nginx --restart=Never --port=80

 ============================================================================================================
 Lables and Selectors:
 ======================
 kubectl run fronend --image=nginx --restart=Never --labels=env=prod,team=amobee --dry-run=client -o yaml

 kubectl run fronend --image=nginx --restart=Never --labels=env=dev,team=turn,app=2.0 --dry-run=client -o yaml

  kubectl run fronend --image=nginx --restart=Never --labels=env=prod,team=noc
 
  kubectl run fronend1 --image=nginx --restart=Never --labels=env=prod,team=techops,app=v2.0
 
  kubectl run fronend3 --image=nginx --restart=Never --labels=env=prod,team=techops,app=amobee
 
 kubectl run fronend4 --image=nginx --restart=Never --labels=env=prod,team=turn
 
 kubectl get pods -l 'team in(noc,techops)'   --show-labels
    
  kubectl get po --show-labels  
   
  kubectl get pods -l 'team in(noc,techops)',env=prod  --show-labels
  
 Annotations
 ============
 
 kubectl annotate pod/fronend contact="noc"
    

=====================================================================================

 
Deployment
==============

kubectl create deployment my-dep --image=nginx --replicas=3 --dry-run=client -o yaml
 
kubectl scale deploy my-dep  --replicas=5

kubectl get po

kubectl describe deploy nginx

kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80

kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80 --dry-run=client  -o yaml > a.yml

kubectl scale --replicas=7 rs nginx

kubectl edit rs nginx

kubectl get rc
kubectl get deploy
kubectl get deploy nginx -o yaml
============================================================================================================
Rollback:
=========

Kubernetes Rolling:
Check how the deployment rollout is going
kubectl rollout status deploy nginx

kubectl set image deploy nginx nginx=nginx:1.19.8

kubectl edit deploy nginx

Check the rollout history and confirm that the replicas are OK

kubectl rollout history deploy nginx
kubectl rollout undo deploy nginx

kubectl get deploy nginx
kubectl get rs  # check that a new replica set has been created
kubectl get po
kubectl scale --replicas=7 rs nginx

==============================================================================================================

Scheduler
=======
 
kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod1.yaml

Multicontainers

// create the pod

kubectl create -f simple-pod.yml// access logs

kubectl logs busybox -c busybox1
kubectl logs busybox -c busybox2
kubectl logs busybox -c busybox3// exec into running containers


Mutlicontainer in POD
========================

kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- /bin/bas

images u can use for minimal container
=========================================

alpine
busybox
centos:latest 
ubuntu:20.04

kubectl exec -it busybox -c busybox1 /bin/sh
kubectl exec -it busybox -c busybox2 /bin/sh
kubectl exec -it busybox -c busybox3 /bin/sh



 kubectl run busybox --image=busybox  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml


kubectl exec -it busybox -- /bin/bash


================================================
Environment:

Of course, we have solved this problem. You can use environment variables and configuration files to store this information in a “central location” and reference 
these from our app. When you want to change the configuration, simply change it in the file or change the environment variable, 
and you are good to go! No need to hunt down every location where the data is referenced.



 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

 kubectl exec busybox -- printenv

 kubectl exec -it busybox -- env

 kubectl exec -it busybox -- /bin/sh


==========================================

Config maps

kubectl create cm mycm --from-file=a.txt
kubectl get cm
kubectl describe cm mycm

kubectl create cm mycm --from-file=a.txt --dry-run=client -o yaml

We see this using by creating it using test pod

kubectl create cm variables --from-file=variables

 kubectl run busybox --image=busybox  --env=team=noc  -o yaml --dry-run=client  -- /bin/sh -c   'echo hello world;sleep 3600' > pod2.yaml

kubectl exec -it busybox -- env

kubectl get cm -o yaml

    
kubectl create secret generic sec --from-file=ssh-private=/root/.ssh/known_hosts --from-literal=passphrase=ravikiran

 kubectl create secret generic db-user-pass  --from-file=a.txt  --from-file=pass.txt --dry-run=client -o yaml

kubectl create secret generic sec --from-file=usercred=ravikiran --from-literal=passphrase=ravikiran300 --dry-run=client -o yaml

=====================================================================================================================================


Node node01 Unschedulable
Pods evicted from node01 

#We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable
kubectl drain node01 --ignore-daemonsets

Node01 is Schedulable
kubectl uncordon node01

Run: kubectl get pods -o wide and you will see that there is a single pod scheduled on node01 which is not part of a replicaset.
The drain command will not work in this case. To forcefully drain the node we now have to use the --force flag.

We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: 

A forceful drain of the node will delete any pod that is not part of a replicaset

This means that both nodes have the ability to schedule workloads on them
 kubectl describe nodes controlplane | grep -i taints 
 
 To get latest version of k8 cluster
 kubectl upgrade plan
 
 We will be upgrading the master node first. Drain the master node of workloads and mark it UnSchedulable
 There are daemonsets created in this cluster, especially in the kube-system namespace. To ignore these objects and drain the node, we can make use of the --ignore-daemonsets flag
 kubectl drain controlplane --ignore-daemonsets
 
 
 In order to ensure minimum downtime, upgrade the cluster one node at a time, while moving the workloads to another node. In the upcoming tasks you will get to practice how to do that
 
 Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead
 
 On the controlplane node, run the command run the following commands:

    apt update
    This will update the package lists from the software repository.

    apt install kubeadm=1.20.0-00
    This will install the kubeadm version 1.20

    kubeadm upgrade apply v1.20.0
    This will upgrade kubernetes controlplane. Note that this can take a few minutes.

    apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.

    You may need to restart kubelet after it has been upgraded.
    Run: systemctl restart kubelet

============

Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet. Practice referring to the kubernetes documentation page. Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead

make it schudable again
kubectl uncordon controlplane

====================================
kubectl get po -n kube-system

Look at the ETCD Logs using the command kubectl logs etcd-controlplane -n kube-system or check the image used by the ETCD pod: 

kubectl describe pod etcd-controlplane -n kube-system

At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD
kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'

etc-cert file
kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'

root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'

snapshot
===========

The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.

Store the backup file at location /opt/snapshot-pre-boot.db

se the etcdctl snapshot save command. You will have to make use of additional flags to connect to the ETCD server.
--endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)
--cacert: Mandatory Flag (Absolute Path to the CA certificate file)
--cert: Mandatory Flag (Absolute Path to the Server certificate file)
--key:Mandatory Flag (Absolute Path to the Key file) 

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db


Snapshot saved at /opt/snapshot-pre-boot.db


Restore the snapshot
====================

Restore the etcd to a new directory from the snapshot by using the etcdctl snapshot restore command. Once the directory is restored, update the ETCD configuration to use the restored directory.

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


2021-03-25 23:52:59.608547 I | mvcc: restore compact to 6466
2021-03-25 23:52:59.621400 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32

Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.

Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory /var/lib/etcd-from-backup.

 volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
    
    With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want)

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

    Note: as the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run a watch "docker ps | grep etcd" command to see when the ETCD pod is restarted.

    Note2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

    Note3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.

If you do change --data-dir to /var/lib/etcd-from-backup in the YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

===========================

Taints and Tolerations
=======================

Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule

    Key = spray
    Value = mortein 

kubectl taint nodes node01 spray=mortein:NoSchedule

kubectl describe node01 | grep -i taints

=======

Create another pod named bee with the NGINX image, which has a toleration set to the taint mortein.

    Image name: nginx
    Key: spray
    Value: mortein
    Effect: NoSchedule
    Status: Running
    
    ===
    
    Create another pod named bee with the NGINX image, which has a toleration set to the taint mortein.

    Image name: nginx
    Key: spray 
    
    ==============================
  
  Node Affinity
  ==============
  
Set Node Affinity to the deployment to place the pods on node01 only.

    Name: blue
    Replicas: 3
    Image: nginx
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    Key: color
    values: blue 
    
    apiVersion: v1
kind: Pod
metadata:
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
    
    remove taint:
    check where is taint in mastet or node
    
    kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-
    
    
    ========================
    
     kubectl label node node01 color=blue
     
     Check if controlplane and node01 have any taints on them that will prevent the pods to be scheduled on them. If there are no taints, the pods can be scheduled on either node.
So run the following command to check the taints on both nodes.
kubectl describe node controlplane | grep -i taints
kubectl describe node node01 | grep -i taints





Update the deployment by running kubectl edit deployment blue and add the nodeaffinity section as follows:

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue

==================================
Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.

Use the label - node-role.kubernetes.io/master - set on the controlplane node.

    Name: red
    Replicas: 2
    Image: nginx
    NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
    Key: node-role.kubernetes.io/master
    Use the right operator 



apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
                
                
                
 =========================================================================================================================================
 ==========================================================================================================================================
 
 Volumes
 ======
 volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
    
    
 image: busybox
    name: busybox
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: DirectoryOrCreate
      
      
     =============================================
 
#https://betterprogramming.pub/tutorial-how-to-use-kubernetes-job-and-cronjob-1ef4ffbc8e84

look into this article about jobs and cron jobs
     
     Job:
     kubectl create job1 rav --image=busybox -o yaml  --dry-run=client   -- /bin/sh -c 'date; echo sleeping....; sleep 90s; echo exiting...'


Enforcing a time limit
For e.g., you are running a batch job and it takes too long to finish due to some reason. This might be undesirable. 
You can limit the time for which a Job can continue to run by setting the activeDeadlineSeconds attribute in the spec.

spec:
  activeDeadlineSeconds: 5
  template:

=====

Handling failures

What if there are issues due to container failure (process exited) or Pod failure? Let's try this out by simulating a failure.

In this Job, the container prints the date, sleeps for 5 seconds, and exits with a status 1 to simulate failure.

here set sleep 5

spec:
  backoffLimit: 2
  template:

==================

cronjob
========

kubectl create cronjob busybox --image=busybox --schedule="*/1 * * * *"   -o yaml --dry-run=client -- /bin/sh -c 'date; echo Hello from the Kubernetes cluster'


serives
======
LoadBalacer
#https://github.com/dennyzhang/cheatsheet-kubernetes-A4

kubectl expose deployment/my-demo-website --type=LoadBalancer --dry-run=client -o yaml > svc.yaml
kubectl expose deployment/mydemo --type=LoadBalancer --dry-run=client -o yaml > svc.yaml

NodePort:
========

ClusterIP:
========




     
     

    





 
 
 
